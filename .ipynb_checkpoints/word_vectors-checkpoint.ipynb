{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mapping Print, Charting Enlightenment\n",
    "\n",
    "## Machine Learning Experiment 1: Classifying books by titles and other metadata\n",
    "\n",
    "Team: Rachel Hendery, Tomas Trescak, Katie McDonough, Michael Falk, Simon Burrows\n",
    "\n",
    "## Notebook 2: Converting titles to mean word vectors \n",
    "\n",
    "Author: Michael Falk\n",
    "\n",
    "In this notebook, I experiment with converting each title into a 300-dimensional feature vector using Facebook's pre-trained French word vector model.\n",
    "\n",
    "The model is large (~5GB), so has not been uploaded to the repository. All Facebook's pre-trained models are available here: https://github.com/facebookresearch/fastText/blob/master/pretrained-vectors.md\n",
    "\n",
    "The binary files FB provides do not work with the Gensim package, which makes them hard to use in python. I have compiled a new version of the bin file that does work with Gensim, which substantially decreases the load time. You can use this code to achieve this:\n",
    "\n",
    "```python\n",
    "from gensim.models import KeyedVectors\n",
    "file_path = \"fr_model/wiki.fr.vec\" # locate .vec file\n",
    "word_vectors = KeyedVectors.load_word2vec_format(file_path) # load into python\n",
    "save_path = \"fr_model/french_vectors.bin\" # choose save path\n",
    "word_vectors.save(save_path, binary = True) # save to it\n",
    "```\n",
    "\n",
    "The aim is to allow the model to generalise to unseen titles that have different words, as well as making the features more meaningful for the learning algorithm.\n",
    "\n",
    "NB: If you're running this notebook on windows, you'll need to have a C compiler installed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained word vectors\n",
    "from gensim.models import KeyedVectors\n",
    "import numpy as np\n",
    "\n",
    "file_path = \"fr_model/french_vectors.bin\"\n",
    "word_vectors = KeyedVectors.load_word2vec_format(file_path, binary = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check - does word_vectors return a 300-dimensional vector as intended?\n",
    "# (Actually it is a 1d numpy array)\n",
    "word_vectors[\"Ã©craser\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data and preprocess\n",
    "import pandas as pd # library for manipulating data\n",
    "from nltk.tokenize import wordpunct_tokenize # tokeniser\n",
    "import re # regular expressions\n",
    "\n",
    "data_path = \"data/editions_trimmed.csv\" # locate data file\n",
    "\n",
    "data = pd.read_csv(data_path).dropna()\n",
    "title_strings = [] # initialise list of word vectors\n",
    "for title in data[\"full_book_title\"]: # loop over titles\n",
    "    title = title.lower() # make all letters lower case\n",
    "    tokens = wordpunct_tokenize(title) # tokenise\n",
    "    reg1 = re.compile(\"\\w\") # regex for finding tokens with letters\n",
    "    reg2 = re.compile(\"\\D\") # regex for finding tokens with numbers\n",
    "    filtered = [i for i in tokens if reg1.search(i)] # strip out punctuation\n",
    "    filtered = [i for i in filtered if reg2.search(i)] # strip out numbers\n",
    "    filtered = [i for i in filtered if len(i) > 2] # strip out stopwords TODO: get proper sw list\n",
    "    title_strings.append(filtered) # append to results list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check: how does a random title look?\n",
    "import random\n",
    "title_strings[random.randint(1,len(title_strings))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step could do with some investigation. From my reading, it is apparently legit to calculate the word embeddings for a whole sentence by getting the word vectors for each word and then averaging them. We will see if this enables the model to train effectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get word vectors for each title\n",
    "import numpy as np\n",
    "\n",
    "def get_mean_word_vec(string_list, word_vectors):\n",
    "    '''\n",
    "    params:\n",
    "    \n",
    "        string_list: a list of strings\n",
    "        word_vectors: a KeyedVectors object (gensim)\n",
    "        \n",
    "        dependencies: numpy, gensim\n",
    "    \n",
    "    desc:\n",
    "    \n",
    "    This function takes a list of strings and a KeyedVectors\n",
    "    object as arguments. It first computes the word vectors for\n",
    "    each string in the list, according to the provided model.\n",
    "    It then takes the mean of the all the vectors. It returns a\n",
    "    single vector, whose dimensionality is determined by the\n",
    "    provided model.\n",
    "    '''\n",
    "    n = word_vectors.vector_size # how many dimensions are the word vectors?\n",
    "    W = np.empty((n,0)) # initialise title matrix\n",
    "    \n",
    "    # for tracking errors\n",
    "    count = int()\n",
    "    \n",
    "    for word in string_list: # loop through strings\n",
    "        count = count + 1\n",
    "        try:\n",
    "            w = word_vectors[word].reshape((n,1)) # find the word vector and check dimensions\n",
    "            W = np.c_[W, w] # add as column to title matrix\n",
    "            break\n",
    "        except:\n",
    "            print(count)\n",
    "    \n",
    "    title_vector = np.mean(W, axis = 1).reshape((n, 1)) # take the sum of each row\n",
    "    \n",
    "    # title_vector is an n x 1 feature vector of the whole title\n",
    "    \n",
    "    return title_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through examples and apply the function.\n",
    "\n",
    "# TODO: parallelise this part to speed the whole thing up.\n",
    "\n",
    "n = word_vectors.vector_size # how many dimensions do our training examples have?\n",
    "T = np.empty((0,n)) # initialise design matrix\n",
    "\n",
    "for title in title_strings:\n",
    "    np.r_[T, get_mean_word_vec(title_strings, word_vectors)]\n",
    "    \n",
    "### Causing weird errors!!!!! Need to learn to debug. :("
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
