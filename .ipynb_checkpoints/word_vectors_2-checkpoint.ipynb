{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mapping Print, Charting Enlightenment\n",
    "\n",
    "## Machine Learning Experiment 1: Classifying books by titles and other metadata\n",
    "\n",
    "Team: Rachel Hendery, Tomas Trescak, Katie McDonough, Michael Falk, Simon Burrows\n",
    "\n",
    "## Notebook 4: Converting titles to dense matrices\n",
    "\n",
    "Author: Michael Falk\n",
    "\n",
    "In this notebook, we convert titles to their dense representation, as in [Notebook 2](word_vectors.ipynb). But this time with a difference: instead of taking the mean for each title, we will turn each title into a list of word vectors. Our new model, an LSTM, will be able to learn not only from what words are in the title, but also from their order.\n",
    "\n",
    "As before, we will use Facebook's pretrained FastText French model, available here: https://github.com/facebookresearch/fastText/blob/master/pretrained-vectors.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load necessary libraries\n",
    "from gensim.models import KeyedVectors\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "import pandas as pd # library for manipulating data\n",
    "from nltk.tokenize import wordpunct_tokenize # tokeniser\n",
    "import re # regular expressions\n",
    "from num2words import num2words\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word vector model imported. Import took 36.92 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained word vectors\n",
    "file_path = \"fr_model/french_vectors.bin\"\n",
    "tic = time.process_time()\n",
    "word_vectors = KeyedVectors.load_word2vec_format(file_path, binary = True)\n",
    "toc = time.process_time()\n",
    "print(f\"Word vector model imported. Import took {toc - tic:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model does have vectors for ordinary words like 'pluie'.\n",
      "The model doesn't have vectors for words with capital letters like 'France'.\n",
      "The model does have vectors for proper names like 'voltaire'.\n",
      "The model does have vectors for accented words like 'zéro'.\n",
      "The model doesn't have vectors for digits like '1'.\n",
      "The model does have vectors for numbers expressed in words like 'un'.\n",
      "But it doesn't have vectors for compound numbers like 'quatre-vignt'.\n",
      "The model doesn't have vectors for all puncutation marks like ';'.\n"
     ]
    }
   ],
   "source": [
    "# What words can the model provide a vector for?\n",
    "def does_doesnt(x):\n",
    "    if x == True:\n",
    "        return(\"does\")\n",
    "    else:\n",
    "        return(\"doesn't\")\n",
    "\n",
    "print(f\"The model {does_doesnt('pluie' in word_vectors.vocab)} have vectors for ordinary words like 'pluie'.\")\n",
    "print(f\"The model {does_doesnt('Gendarme' in word_vectors.vocab)} have vectors for words with capital letters like 'France'.\")\n",
    "print(f\"The model {does_doesnt('voltaire' in word_vectors.vocab)} have vectors for proper names like 'voltaire'.\")\n",
    "print(f\"The model {does_doesnt('zéro' in word_vectors.vocab)} have vectors for accented words like 'zéro'.\")\n",
    "print(f\"The model {does_doesnt('1' in word_vectors.vocab)} have vectors for digits like '1'.\")\n",
    "print(f\"The model {does_doesnt('un' in word_vectors.vocab)} have vectors for numbers expressed in words like 'un'.\")\n",
    "print(f\"But it {does_doesnt('quatre-vignt' in word_vectors.vocab)} have vectors for compound numbers like 'quatre-vignt'.\")\n",
    "print(f\"The model {does_doesnt(';' in word_vectors.vocab)} have vectors for all puncutation marks like ';'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to do some light preprocessing of the data to account for these omissions of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper function for preprocessing\n",
    "def conv_tok(string):\n",
    "    # Takes a string containing digits, converts them to strings and tokenises.\n",
    "    \n",
    "    # First, need to handle ordinals, which have an 'e' on the end of them in French.\n",
    "    if re.search('e|è', string): # if there is an 'e' or 'è' ...\n",
    "        string = re.sub('\\D', '', string) # ... remove it, and everything else that isn't a digit.\n",
    "        num = int(string) # convert the string to an integer ...\n",
    "        phrase = num2words(num, lang = 'fr', to = 'ordinal') # ... and then into a phrase\n",
    "    else: # if there is no 'e' ...\n",
    "        string = re.sub('\\D', '', string) # ... delete any letters anyway ...\n",
    "        num = int(string)\n",
    "        phrase = num2words(num, lang = 'fr') # ... and don't treat it as an ordinal\n",
    "    \n",
    "    tokens = wordpunct_tokenize(phrase)\n",
    "    reg1 = re.compile(\"\\w\") # only word characters\n",
    "    words = [i for i in tokens if reg1.search(i)] # drop puncuation (e.g. hyphens)\n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data comprises 3838 books described by 4 fields.\n"
     ]
    }
   ],
   "source": [
    "# Import the title data\n",
    "data_path = \"data/editions_trimmed.csv\" # locate data file\n",
    "\n",
    "data = pd.read_csv(data_path).dropna() # read in title data\n",
    "\n",
    "print(f\"The data comprises {data.shape[0]} books described by {data.shape[1]} fields.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_strings = [] # initialise list of word vectors\n",
    "\n",
    "letters = re.compile(\"\\w\") # regex for finding tokens with letters\n",
    "digits = re.compile(\"\\d\") # regex for finding tokens with numbers\n",
    "\n",
    "for i, title in enumerate(data[\"full_book_title\"]): # loop over titles\n",
    "    title = title.lower() # make all letters lower case\n",
    "    tokens = wordpunct_tokenize(title) # tokenise\n",
    "    \n",
    "    # Dealing with numbers:\n",
    "    for counter, token in enumerate(tokens): # loop over tokens in this title\n",
    "        if digits.search(token): # if a numeral is found ...\n",
    "            new_list = conv_tok(token) # ... convert it to a list of verbalised numbers\n",
    "            tokens.remove(token) # remove the discovered digits from the token list for this title\n",
    "            new_list.reverse() # reverse the list\n",
    "            for num in new_list: # loop through verbalised numbers backwards ...\n",
    "                tokens.insert(counter, num) # ... and add the new strings to the list\n",
    "    \n",
    "    # Final cleanup:\n",
    "    filtered = [i for i in tokens if letters.search(i)] # only keep letters\n",
    "    filtered = [i for i in filtered if len(i) > 2] # strip out stopwords TODO: get proper sw list\n",
    "    \n",
    "    # Complete loop:\n",
    "    title_strings.append(filtered) # append to results list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['contemplation', 'nature']"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity check: how does a random title look?\n",
    "title_strings[random.randint(0, len(title_strings))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The longest title is 59 words long\n",
      "There are 194 titles with more than 25 words, out of 3838.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([1864., 1040.,  475.,  228.,  101.,   65.,   32.,   14.,   11.,\n",
       "           8.]),\n",
       " array([ 1. ,  6.8, 12.6, 18.4, 24.2, 30. , 35.8, 41.6, 47.4, 53.2, 59. ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Now for something more important: how long is each title?\n",
    "lengths = np.array([len(i) for i in title_strings])\n",
    "print(f\"The longest title is {lengths.max()} words long\")\n",
    "thresh = 25\n",
    "print(f\"There are {lengths[np.where(lengths > thresh)].shape[0]} titles with more than {thresh} words, out of {lengths.shape[0]}.\")\n",
    "pyplot.hist(lengths)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
